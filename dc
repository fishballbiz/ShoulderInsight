#!/bin/bash

# ./dc utility script

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
if [ -f "$SCRIPT_DIR/.env.local" ]; then
    set -a
    source "$SCRIPT_DIR/.env.local"
    set +a
fi

COMPOSE_FILE="docker-compose.yml"
SERVICE_NAME="web"

GCP_PROJECT="aishoulder"
GCP_REGION="asia-east1"
IMAGE_NAME="aishoulder"
REGISTRY="${GCP_REGION}-docker.pkg.dev/${GCP_PROJECT}/${IMAGE_NAME}/${IMAGE_NAME}"
STATIC_BUCKET="${GCP_PROJECT}-static"

function show_help {
    echo "Usage: ./dc [command]"
    echo "Commands:"
    echo "  build dev       Build the docker image"
    echo "  dev             Enter the development container (bash)"
    echo "  up              Start services"
    echo "  down            Stop services"
    echo "  logs            View logs"
    echo "  manage [args]   Run python manage.py [args]"
    echo "  publish         Deploy to GCP Cloud Run"
    echo "  init-repo       Initialize .env.local with required settings"
    echo "  help            Show this help message"
}

if [ -z "$1" ]; then
    show_help
    exit 1
fi

case "$1" in
    "build")
        if [ "$2" == "dev" ]; then
            docker-compose -f $COMPOSE_FILE build
        else
            docker-compose -f $COMPOSE_FILE build "$@"
        fi
        ;;
    "dev")
        # Ensure services are up or run a one-off container
        # If we want to "enter" the image, we usually mean running bash
        docker-compose -f $COMPOSE_FILE run --rm --service-ports $SERVICE_NAME bash
        ;;
    "up")
        docker-compose -f $COMPOSE_FILE up -d --remove-orphans
        ;;
    "down")
        docker-compose -f $COMPOSE_FILE down --remove-orphans
        ;;
    "logs")
        docker-compose -f $COMPOSE_FILE logs -f
        ;;
    "manage")
        shift
        docker-compose -f $COMPOSE_FILE run --rm $SERVICE_NAME python manage.py "$@"
        ;;
    "init-repo")
        ENV_FILE="$SCRIPT_DIR/.env.local"
        if [ -f "$ENV_FILE" ]; then
            echo ".env.local already exists at $ENV_FILE"
            exit 0
        fi
        GENERATED_KEY=$(python3 -c "import secrets; print(secrets.token_urlsafe(50))")
        cat > "$ENV_FILE" <<ENVEOF
EXPECTED_ACCOUNT=your-gcp-account@gmail.com
SECRET_KEY=$GENERATED_KEY
GEMINI_API_KEY=your-gemini-api-key
ENVEOF
        echo "Created $ENV_FILE"
        echo "SECRET_KEY has been auto-generated."
        echo "Edit EXPECTED_ACCOUNT and GEMINI_API_KEY before running ./dc publish"
        ;;
    "publish")
        # Check GCP account
        CURRENT_ACCOUNT=$(gcloud config get-value account 2>/dev/null)
        if [ -z "$EXPECTED_ACCOUNT" ]; then
            echo "Error: EXPECTED_ACCOUNT not set. Run: ./dc init-repo"
            exit 1
        fi
        if [ -z "$SECRET_KEY" ]; then
            echo "Error: SECRET_KEY not set. Run: ./dc init-repo"
            exit 1
        fi
        if [ -z "$GEMINI_API_KEY" ]; then
            echo "Error: GEMINI_API_KEY not set. Add it to .env.local"
            exit 1
        fi
        if [ "$CURRENT_ACCOUNT" != "$EXPECTED_ACCOUNT" ]; then
            echo "Error: Wrong GCP account. Expected $EXPECTED_ACCOUNT, got $CURRENT_ACCOUNT"
            echo "Run: gcloud config set account $EXPECTED_ACCOUNT"
            exit 1
        fi
        echo "Using GCP account: $CURRENT_ACCOUNT"

        # Set project
        gcloud config set project $GCP_PROJECT

        # Create Artifact Registry repository if not exists
        gcloud artifacts repositories describe $IMAGE_NAME \
            --location=$GCP_REGION 2>/dev/null || \
        gcloud artifacts repositories create $IMAGE_NAME \
            --repository-format=docker \
            --location=$GCP_REGION

        # Create GCS bucket for static files if not exists
        echo "Setting up static files bucket..."
        if ! gcloud storage buckets describe gs://$STATIC_BUCKET 2>/dev/null; then
            gcloud storage buckets create gs://$STATIC_BUCKET \
                --location=$GCP_REGION \
                --uniform-bucket-level-access
        fi

        # Make bucket public for static files
        gcloud storage buckets add-iam-policy-binding gs://$STATIC_BUCKET \
            --member=allUsers \
            --role=roles/storage.objectViewer 2>/dev/null || true

        # Collect and upload static files
        echo "Collecting static files..."
        docker-compose -f $COMPOSE_FILE run --rm $SERVICE_NAME python manage.py collectstatic --noinput

        echo "Uploading static files to GCS..."
        gcloud storage rsync staticfiles/ gs://$STATIC_BUCKET/static/ \
            --recursive \
            --delete-unmatched-destination-objects

        # Set cache headers for static files
        gcloud storage objects update gs://$STATIC_BUCKET/static/** \
            --cache-control="public, max-age=86400" 2>/dev/null || true

        STATIC_URL="https://storage.googleapis.com/$STATIC_BUCKET/static/"

        # Build and push image
        echo "Building and pushing image..."
        docker build --platform linux/amd64 -t $REGISTRY:latest .
        docker push $REGISTRY:latest

        # Deploy to Cloud Run with minimal resources
        echo "Deploying to Cloud Run..."
        gcloud run deploy $IMAGE_NAME \
            --image=$REGISTRY:latest \
            --platform=managed \
            --region=$GCP_REGION \
            --allow-unauthenticated \
            --min-instances=0 \
            --max-instances=1 \
            --memory=512Mi \
            --cpu=1 \
            --concurrency=10 \
            --timeout=60 \
            --cpu-boost \
            --set-env-vars="STATIC_URL=$STATIC_URL,SECRET_KEY=$SECRET_KEY,GEMINI_API_KEY=$GEMINI_API_KEY"

        # Clean up old images (keep only latest)
        echo "Cleaning up old images..."
        gcloud artifacts docker images list $REGISTRY \
            --include-tags \
            --format="get(version)" \
            --filter="NOT tags:latest" | \
        while read -r digest; do
            if [ -n "$digest" ]; then
                gcloud artifacts docker images delete "${REGISTRY}@${digest}" --quiet 2>/dev/null || true
            fi
        done

        echo "Deployment complete!"
        echo "Static files: $STATIC_URL"
        gcloud run services describe $IMAGE_NAME --region=$GCP_REGION --format="value(status.url)"
        ;;
    *)
        # Pass through other commands to docker-compose
        docker-compose -f $COMPOSE_FILE "$@"
        ;;
esac
